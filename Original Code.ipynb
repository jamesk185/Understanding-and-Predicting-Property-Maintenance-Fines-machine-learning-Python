{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (11,12,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# loading in the data\n",
    "training_data = pd.read_csv('readonly/train.csv', encoding='latin1')\n",
    "testing_data = pd.read_csv('readonly/test.csv')\n",
    "address_data = pd.read_csv('readonly/addresses.csv')\n",
    "latlon_data = pd.read_csv('readonly/latlons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing variables unavailable in test set\n",
    "unavailable_vars = ['payment_amount', 'payment_date', 'payment_status', \n",
    "                   'balance_due', 'collection_status', 'compliance_detail']\n",
    "training_data.drop(unavailable_vars, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_id                     0.000000\n",
       "agency_name                   0.000000\n",
       "inspector_name                0.000000\n",
       "violator_name                 0.000136\n",
       "violation_street_number       0.000000\n",
       "violation_street_name         0.000000\n",
       "violation_zip_code            1.000000\n",
       "mailing_address_str_number    0.014390\n",
       "mailing_address_str_name      0.000016\n",
       "city                          0.000000\n",
       "state                         0.000372\n",
       "zip_code                      0.000004\n",
       "non_us_str_code               0.999988\n",
       "country                       0.000000\n",
       "ticket_issued_date            0.000000\n",
       "hearing_date                  0.049903\n",
       "violation_code                0.000000\n",
       "violation_description         0.000000\n",
       "disposition                   0.000000\n",
       "fine_amount                   0.000004\n",
       "admin_fee                     0.000000\n",
       "state_fee                     0.000000\n",
       "late_fee                      0.000000\n",
       "discount_amount               0.000000\n",
       "clean_up_cost                 0.000000\n",
       "judgment_amount               0.000000\n",
       "grafitti_status               0.999996\n",
       "compliance                    0.361262\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examining percentage of null occurrences in each variable\n",
    "training_data.isnull().sum()/len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing null dominant variables\n",
    "training_data.drop(['violation_zip_code', 'grafitti_status', 'non_us_str_code'], \n",
    "                   inplace=True, axis=1)\n",
    "testing_data.drop(['violation_zip_code', 'grafitti_status', 'non_us_str_code'], \n",
    "                   inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing rows where compliance is null\n",
    "training_data = training_data.loc[~training_data['compliance'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assuming all data is from Detroit and thus removing unnecessary variables\n",
    "# while noting that some relevant address info may be capture in lat/lon data\n",
    "training_data.drop(['city', 'state', 'zip_code', 'country'],\n",
    "                   inplace=True, axis=1)\n",
    "testing_data.drop(['city', 'state', 'zip_code', 'country'],\n",
    "                   inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding lat and lon variables by merging while removing address variable\n",
    "addlanlon_data = address_data.merge(latlon_data, how='inner', on='address')\n",
    "addlanlon_data.drop('address', inplace=True, axis=1)\n",
    "training_data = training_data.merge(addlanlon_data, how='inner', on='ticket_id')\n",
    "testing_data = testing_data.merge(addlanlon_data, how='inner', on='ticket_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating new binomial variable that indicates whether the mailing address and\n",
    "# violation address street name are the same \n",
    "# then removing unnecessary variables\n",
    "training_data['violadd_equal_mailadd'] = training_data['violation_street_name']==training_data['mailing_address_str_name']\n",
    "training_data['violadd_equal_mailadd'] = training_data['violadd_equal_mailadd'].astype('uint8')\n",
    "training_data.drop(['violation_street_number', 'violation_street_name',\n",
    "                   'mailing_address_str_number', 'mailing_address_str_name'],\n",
    "                  inplace=True, axis=1)\n",
    "\n",
    "testing_data['violadd_equal_mailadd'] = testing_data['violation_street_name']==testing_data['mailing_address_str_name']\n",
    "testing_data['violadd_equal_mailadd'] = testing_data['violadd_equal_mailadd'].astype('uint8')\n",
    "testing_data.drop(['violation_street_number', 'violation_street_name',\n",
    "                   'mailing_address_str_number', 'mailing_address_str_name'],\n",
    "                  inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_id                  int64\n",
       "agency_name               object\n",
       "inspector_name            object\n",
       "violator_name             object\n",
       "ticket_issued_date        object\n",
       "hearing_date              object\n",
       "violation_code            object\n",
       "violation_description     object\n",
       "disposition               object\n",
       "fine_amount              float64\n",
       "admin_fee                float64\n",
       "state_fee                float64\n",
       "late_fee                 float64\n",
       "discount_amount          float64\n",
       "clean_up_cost            float64\n",
       "judgment_amount          float64\n",
       "compliance               float64\n",
       "lat                      float64\n",
       "lon                      float64\n",
       "violadd_equal_mailadd      uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at data types, with some quick examination of non-numeric variables\n",
    "training_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: modifications to a property of a datetimelike object are not supported and are discarded. Change values on the original.\n",
      "/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py:4702: SettingWithCopyWarning: modifications to a property of a datetimelike object are not supported and are discarded. Change values on the original.\n",
      "  self._update_inplace(new_data)\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:25: SettingWithCopyWarning: modifications to a property of a datetimelike object are not supported and are discarded. Change values on the original.\n"
     ]
    }
   ],
   "source": [
    "# I see that violation description can be removed whilst date variables must be\n",
    "# converted to a datetime format and can be merged to one variable\n",
    "# note that there are NaN hearing dates so I have replaced these day count\n",
    "# differences with the mean day difference of the whole dataset\n",
    "training_data.drop('violation_description', inplace=True, axis=1)\n",
    "training_data['ticket_issued_date'] = pd.to_datetime(training_data['ticket_issued_date'] , \n",
    "                                                     format='%Y/%m/%d %H:%M:%S')\n",
    "training_data['hearing_date'] = pd.to_datetime(training_data['hearing_date'] , \n",
    "                                                     format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "x = (training_data['hearing_date']-training_data['ticket_issued_date']).dt.days\n",
    "xmean = x[x>0].mean()\n",
    "x[x.isnull()] = xmean\n",
    "training_data['days_from_issue_to_hearing'] = x\n",
    "training_data.drop(['hearing_date'], inplace=True, axis=1)\n",
    "\n",
    "testing_data.drop('violation_description', inplace=True, axis=1)\n",
    "testing_data['ticket_issued_date'] = pd.to_datetime(testing_data['ticket_issued_date'] , \n",
    "                                                     format='%Y/%m/%d %H:%M:%S')\n",
    "testing_data['hearing_date'] = pd.to_datetime(testing_data['hearing_date'] , \n",
    "                                                     format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "x = (testing_data['hearing_date']-testing_data['ticket_issued_date']).dt.days\n",
    "xmean = x[x>0].mean()\n",
    "x[x.isnull()] = xmean\n",
    "testing_data['days_from_issue_to_hearing'] = x\n",
    "testing_data.drop(['hearing_date'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dealing with more variables I presume inspector_name can be removed and I look\n",
    "# to create new variable from violator_name which indicates whether the violator\n",
    "# has a previous record or is a first time offender\n",
    "training_data['prev_offence'] = np.nan\n",
    "training_data = training_data.sort_values(by='ticket_issued_date')\n",
    "\n",
    "for i in range(0, len(training_data)):\n",
    "    name = training_data.iloc[i]['violator_name']\n",
    "    all_names = list(training_data.iloc[:i]['violator_name'])\n",
    "    if name in all_names :\n",
    "        training_data.iloc[i, training_data.columns.get_loc('prev_offence')] = 1\n",
    "    else :\n",
    "        training_data.iloc[i, training_data.columns.get_loc('prev_offence')] = 0\n",
    "        \n",
    "testing_data['prev_offence'] = np.nan\n",
    "testing_data = testing_data.sort_values(by='ticket_issued_date')\n",
    "training_names = training_data['violator_name'].unique()\n",
    "\n",
    "for i in range(0, len(testing_data)):\n",
    "    name = testing_data.iloc[i]['violator_name']\n",
    "    all_names = list(testing_data.iloc[:i]['violator_name'].unique())\n",
    "    if name in all_names :\n",
    "        testing_data.iloc[i, testing_data.columns.get_loc('prev_offence')] = 1\n",
    "    elif name in training_names :\n",
    "        testing_data.iloc[i, testing_data.columns.get_loc('prev_offence')] = 1\n",
    "    else :\n",
    "        testing_data.iloc[i, testing_data.columns.get_loc('prev_offence')] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving the dataset for quicker loading\n",
    "training_data.to_csv('training_data2.csv')\n",
    "testing_data.to_csv('testing_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data2 = pd.read_csv('training_data2.csv')\n",
    "testing_data2 = pd.read_csv('testing_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dropping unnecessary variables\n",
    "training_data2.drop(['inspector_name', 'violator_name', 'ticket_issued_date'], \n",
    "                    inplace=True, axis=1)\n",
    "testing_data2.drop(['inspector_name', 'violator_name', 'ticket_issued_date'], \n",
    "                    inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean_up_cost, state_fee and admin_fee all contain the same values so can be \n",
    "# removed\n",
    "training_data2.drop(['clean_up_cost', 'admin_fee', 'state_fee', 'Unnamed: 0'], \n",
    "                    inplace=True, axis=1)\n",
    "training_data2.set_index('ticket_id', inplace=True)\n",
    "testing_data2.drop(['clean_up_cost', 'admin_fee', 'state_fee', 'Unnamed: 0'], \n",
    "                    inplace=True, axis=1)\n",
    "testing_data2.set_index('ticket_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# disposition and agency_name variables are categorical so can be converted\n",
    "# into sets of binomial variables respectively\n",
    "training_data2 = pd.get_dummies(training_data2, columns = ['agency_name', 'disposition'])\n",
    "testing_data2 = pd.get_dummies(testing_data2, columns = ['agency_name', 'disposition'])\n",
    "training_data2['disposition_Responsible - Compl/Adj by Default'] = 0\n",
    "training_data2['disposition_Responsible - Compl/Adj by Determi'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# violation_code is a variable of interest since the type of violation could \n",
    "# absolutely be a strong indicator of compliance. However, with too many unique\n",
    "# values and the potential for new violation codes to be created and appear, the\n",
    "# variable will have to be ignored\n",
    "training_data2.drop('violation_code', inplace=True, axis=1)\n",
    "testing_data2.drop('violation_code', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing judgment_amount as this information is captured in the other \n",
    "# variables concerning fees\n",
    "training_data2.drop('judgment_amount', inplace=True, axis=1)\n",
    "testing_data2.drop('judgment_amount', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fine_amount                                                  -0.049134\n",
       "late_fee                                                     -0.085055\n",
       "discount_amount                                               0.156073\n",
       "compliance                                                    1.000000\n",
       "lat                                                          -0.021569\n",
       "lon                                                          -0.000431\n",
       "violadd_equal_mailadd                                         0.008933\n",
       "days_from_issue_to_hearing                                   -0.004666\n",
       "prev_offence                                                 -0.093310\n",
       "agency_name_Buildings, Safety Engineering & Env Department   -0.055637\n",
       "agency_name_Department of Public Works                        0.046939\n",
       "agency_name_Detroit Police Department                         0.038672\n",
       "agency_name_Health Department                                -0.005559\n",
       "agency_name_Neighborhood City Halls                          -0.000699\n",
       "disposition_Responsible (Fine Waived) by Deter                0.124956\n",
       "disposition_Responsible by Admission                          0.238997\n",
       "disposition_Responsible by Default                           -0.335455\n",
       "disposition_Responsible by Determination                      0.202819\n",
       "disposition_Responsible - Compl/Adj by Default                     NaN\n",
       "disposition_Responsible - Compl/Adj by Determi                     NaN\n",
       "Name: compliance, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at variable correlations with compliance there doesn't seem to be any \n",
    "# issues\n",
    "training_data2.corr()['compliance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fine_amount                                                   0.000000\n",
       "late_fee                                                      0.000000\n",
       "discount_amount                                               0.000000\n",
       "compliance                                                    0.000000\n",
       "lat                                                           0.000013\n",
       "lon                                                           0.000013\n",
       "violadd_equal_mailadd                                         0.000000\n",
       "days_from_issue_to_hearing                                    0.000000\n",
       "prev_offence                                                  0.000000\n",
       "agency_name_Buildings, Safety Engineering & Env Department    0.000000\n",
       "agency_name_Department of Public Works                        0.000000\n",
       "agency_name_Detroit Police Department                         0.000000\n",
       "agency_name_Health Department                                 0.000000\n",
       "agency_name_Neighborhood City Halls                           0.000000\n",
       "disposition_Responsible (Fine Waived) by Deter                0.000000\n",
       "disposition_Responsible by Admission                          0.000000\n",
       "disposition_Responsible by Default                            0.000000\n",
       "disposition_Responsible by Determination                      0.000000\n",
       "disposition_Responsible - Compl/Adj by Default                0.000000\n",
       "disposition_Responsible - Compl/Adj by Determi                0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for nans one last time shows the presence of some in the lat and lon \n",
    "# variables.\n",
    "training_data2.isnull().sum()/len(training_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replacing nans with the mean\n",
    "latmean = np.mean(training_data2['lat'])\n",
    "lonmean = np.mean(training_data2['lon'])\n",
    "nanlatlon = list(training_data2[training_data2['lat'].isnull()].index)\n",
    "for i in nanlatlon:\n",
    "    training_data2.loc[i, 'lat'] = latmean\n",
    "    training_data2.loc[i, 'lon'] = lonmean\n",
    "\n",
    "nanlatlon = list(testing_data2[testing_data2['lat'].isnull()].index)\n",
    "for i in nanlatlon:\n",
    "    testing_data2.loc[i, 'lat'] = latmean\n",
    "    testing_data2.loc[i, 'lon'] = lonmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now for the prediction models. First I will split the training set into \n",
    "# train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "traindata = training_data2.drop('compliance', axis=1)\n",
    "traincompliance = training_data2['compliance']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(traindata, traincompliance,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.76281538,  0.77621997,  0.77689392,  0.77946438,  0.78377937])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first trying a Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "nbclf = GaussianNB().fit(X_train, y_train)\n",
    "cross_val_score(nbclf, X_test, y_test, cv=5, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7497346 ,  0.74796527,  0.7386544 ,  0.75410886,  0.74572621])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next trying random forests model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfclf = RandomForestRegressor(random_state=0).fit(X_train, y_train)\n",
    "cross_val_score(rfclf, X_test, y_test, cv=5, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.80726241,  0.80475678,  0.81073189,  0.81416076,  0.82483667])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next trying a gradient boosted decision tree model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbclf = GradientBoostingRegressor(random_state=0).fit(X_train, y_train)\n",
    "cross_val_score(gbclf, X_test, y_test, cv=5, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.3, 'n_estimators': 50}\n",
      "0.820923386309\n"
     ]
    }
   ],
   "source": [
    "# using grid_values to find good parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_values = {'n_estimators': [50,75,100], \n",
    "               'learning_rate': [0.01, 0.1, 0.3, 0.4]}\n",
    "grid_gbclf = GridSearchCV(gbclf, param_grid = grid_values, scoring='roc_auc')\n",
    "grid_gbclf.fit(X_train, y_train)\n",
    "print(grid_gbclf.best_params_)\n",
    "print(grid_gbclf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GradientBoostingRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f0da922dd2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m gbclf = GradientBoostingRegressor(random_state=0,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                   \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   learning_rate=0.3).fit(X_train, y_train)\n\u001b[1;32m      4\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GradientBoostingRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "gbclf = GradientBoostingRegressor(random_state=0,\n",
    "                                  n_estimators=50,\n",
    "                                  learning_rate=0.3).fit(X_train, y_train)\n",
    "cross_val_score(gbclf, X_test, y_test, cv=5, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5}\n",
      "0.82173473978\n"
     ]
    }
   ],
   "source": [
    "grid_values = {'max_depth': [3,4,5]}\n",
    "grid_gbclf = GridSearchCV(gbclf, param_grid = grid_values, scoring='roc_auc')\n",
    "grid_gbclf.fit(X_train, y_train)\n",
    "print(grid_gbclf.best_params_)\n",
    "print(grid_gbclf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.80116443,  0.80810559,  0.80593239,  0.80971667,  0.80993976])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "gbclf = GradientBoostingRegressor(random_state=0,\n",
    "                                  n_estimators=50,\n",
    "                                  learning_rate=0.3,\n",
    "                                 max_depth=5).fit(X_train, y_train)\n",
    "cross_val_score(gbclf, X_test, y_test, cv=5, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_id\n",
       "284932    0.168202\n",
       "285343    0.144434\n",
       "285344    0.365637\n",
       "285362    0.120585\n",
       "285342    0.880129\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = gbclf.predict(testing_data2)\n",
    "y_preds = pd.Series(y_preds, index=list(testing_data2.index))\n",
    "y_preds = y_preds.rename_axis('ticket_id')\n",
    "y_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# due to the function timing out in the auto-grader, the process needs speeding\n",
    "# up and as prev_offence is the slowest process it should be removed\n",
    "\n",
    "def blight_model():\n",
    "    \n",
    "    training_data = pd.read_csv('train.csv', encoding='latin1')\n",
    "    testing_data = pd.read_csv('test.csv')\n",
    "    address_data = pd.read_csv('addresses.csv')\n",
    "    latlon_data = pd.read_csv('latlons.csv')\n",
    "    \n",
    "    unavailable_vars = ['payment_amount', 'payment_date', 'payment_status', \n",
    "                   'balance_due', 'collection_status', 'compliance_detail']\n",
    "    training_data.drop(unavailable_vars, inplace=True, axis=1)\n",
    "    \n",
    "    training_data.drop(['violation_zip_code', 'grafitti_status', 'non_us_str_code',\n",
    "                       'city', 'state', 'zip_code', 'country'], \n",
    "                   inplace=True, axis=1)\n",
    "    testing_data.drop(['violation_zip_code', 'grafitti_status', 'non_us_str_code',\n",
    "                      'city', 'state', 'zip_code', 'country'], \n",
    "                   inplace=True, axis=1)\n",
    "    \n",
    "    training_data = training_data.loc[~training_data['compliance'].isnull()]\n",
    "    \n",
    "    addlanlon_data = address_data.merge(latlon_data, how='inner', on='address')\n",
    "    addlanlon_data.drop('address', inplace=True, axis=1)\n",
    "    training_data = training_data.merge(addlanlon_data, how='inner', on='ticket_id')\n",
    "    testing_data = testing_data.merge(addlanlon_data, how='inner', on='ticket_id')\n",
    "    \n",
    "    training_data['violadd_equal_mailadd'] = training_data['violation_street_name']==training_data['mailing_address_str_name']\n",
    "    training_data['violadd_equal_mailadd'] = training_data['violadd_equal_mailadd'].astype('uint8')\n",
    "    training_data.drop(['violation_street_number', 'violation_street_name',\n",
    "                   'mailing_address_str_number', 'mailing_address_str_name'],\n",
    "                  inplace=True, axis=1)\n",
    "\n",
    "    testing_data['violadd_equal_mailadd'] = testing_data['violation_street_name']==testing_data['mailing_address_str_name']\n",
    "    testing_data['violadd_equal_mailadd'] = testing_data['violadd_equal_mailadd'].astype('uint8')\n",
    "    testing_data.drop(['violation_street_number', 'violation_street_name',\n",
    "                   'mailing_address_str_number', 'mailing_address_str_name'],\n",
    "                  inplace=True, axis=1)\n",
    "    \n",
    "    training_data.drop('violation_description', inplace=True, axis=1)\n",
    "    training_data['ticket_issued_date'] = pd.to_datetime(training_data['ticket_issued_date'] , \n",
    "                                                     format='%Y/%m/%d %H:%M:%S')\n",
    "    training_data['hearing_date'] = pd.to_datetime(training_data['hearing_date'] , \n",
    "                                                     format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "    x = (training_data['hearing_date']-training_data['ticket_issued_date']).dt.days\n",
    "    xmean = x[x>0].mean()\n",
    "    x[x.isnull()] = xmean\n",
    "    training_data['days_from_issue_to_hearing'] = x\n",
    "    training_data.drop(['hearing_date'], inplace=True, axis=1)\n",
    "\n",
    "    testing_data.drop('violation_description', inplace=True, axis=1)\n",
    "    testing_data['ticket_issued_date'] = pd.to_datetime(testing_data['ticket_issued_date'] , \n",
    "                                                     format='%Y/%m/%d %H:%M:%S')\n",
    "    testing_data['hearing_date'] = pd.to_datetime(testing_data['hearing_date'] , \n",
    "                                                     format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "    x = (testing_data['hearing_date']-testing_data['ticket_issued_date']).dt.days\n",
    "    xmean = x[x>0].mean()\n",
    "    x[x.isnull()] = xmean\n",
    "    testing_data['days_from_issue_to_hearing'] = x\n",
    "    testing_data.drop(['hearing_date'], inplace=True, axis=1)\n",
    "    \n",
    "    training_data.drop(['inspector_name', 'violator_name', 'ticket_issued_date'], \n",
    "                    inplace=True, axis=1)\n",
    "    testing_data.drop(['inspector_name', 'violator_name', 'ticket_issued_date'], \n",
    "                    inplace=True, axis=1)\n",
    "\n",
    "    training_data.drop(['clean_up_cost', 'admin_fee', 'state_fee'], \n",
    "                    inplace=True, axis=1)\n",
    "    training_data.set_index('ticket_id', inplace=True)\n",
    "    testing_data.drop(['clean_up_cost', 'admin_fee', 'state_fee'], \n",
    "                    inplace=True, axis=1)\n",
    "    testing_data.set_index('ticket_id', inplace=True)\n",
    "    \n",
    "    training_data = pd.get_dummies(training_data, columns = ['agency_name', 'disposition'])\n",
    "    testing_data = pd.get_dummies(testing_data, columns = ['agency_name', 'disposition'])\n",
    "    training_data['disposition_Responsible - Compl/Adj by Default'] = 0\n",
    "    training_data['disposition_Responsible - Compl/Adj by Determi'] = 0\n",
    "    \n",
    "    training_data.drop(['violation_code', 'judgment_amount'], inplace=True, axis=1)\n",
    "    testing_data.drop(['violation_code', 'judgment_amount'], inplace=True, axis=1)\n",
    "    \n",
    "    latmean = np.mean(training_data['lat'])\n",
    "    lonmean = np.mean(training_data['lon'])\n",
    "    nanlatlon = list(training_data[training_data['lat'].isnull()].index)\n",
    "    for i in nanlatlon:\n",
    "        training_data.loc[i, 'lat'] = latmean\n",
    "        training_data.loc[i, 'lon'] = lonmean\n",
    "\n",
    "    nanlatlon = list(testing_data[testing_data['lat'].isnull()].index)\n",
    "    for i in nanlatlon:\n",
    "        testing_data.loc[i, 'lat'] = latmean\n",
    "        testing_data.loc[i, 'lon'] = lonmean\n",
    "        \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    traindata = training_data.drop('compliance', axis=1)\n",
    "    traincompliance = training_data['compliance']\n",
    "    \n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(traindata, traincompliance,\n",
    "                                                    random_state=0)\n",
    "    gbclf = GradientBoostingRegressor(random_state=0,\n",
    "                                  n_estimators=50,\n",
    "                                  learning_rate=0.3,\n",
    "                                  max_depth=5).fit(X_train, y_train)\n",
    "    \n",
    "    y_preds = gbclf.predict(testing_data)\n",
    "    y_preds = pd.Series(y_preds, index=list(testing_data.index))\n",
    "    y_preds = y_preds.rename_axis('ticket_id')\n",
    "    \n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your AUC of 0.782910006428 was awarded a value of 1.0 out of 1.0 total grades\n",
    "blight_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
